{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | NLP | LLM | LangChain | Multi-Step Reasoning 1 |\n\n## Natural Language Processing (NLP) and Large Language Models (LLM) with LangChain and Building Multi-stage Reasoning Systems\n\n![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)\n\n\n# <b>1 <span style='color:#78D118'>|</span> Overview</b>\n\nIn this notebook we're going to create AI systems:\n- Named `CommentatorMoodModeratorAI` will be a prototype AI self-commenting-and-moderating tool that will create new reaction comments to a piece of text with one LLM and use another LLM to critique those comments and flag them if they are negative. To build this we will walk through the steps needed to construct prompts and chains, as well as multiple LLM Chains that take multiple inputs, both from the previous LLM and external. \n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n1. Build prompt template and create new prompts with different inputs\n2. Create basic LLM chains to connect prompts and LLMs.\n3. Construct sequential chains of multiple `LLMChains` to perform multi-stage reasoning analysis. \n4. Use langchain agents to build semi-automated systems with an LLM-centric agent.\n\n\n<img src=\"https://deepsense.ai/wp-content/uploads/2023/10/LangChain-announces-partnership-with-deepsense.jpeg\" alt=\"Learning\" width=\"50%\">\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Setup\n","metadata":{}},{"cell_type":"code","source":"!pip install openai==1.6.1 httpcore==1.0.2 httpx==0.26.0 typing-extensions==4.9.0 pydantic==1.9.0 better-profanity langchain","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:44:06.493274Z","iopub.execute_input":"2023-12-30T22:44:06.494314Z","iopub.status.idle":"2023-12-30T22:44:33.161180Z","shell.execute_reply.started":"2023-12-30T22:44:06.494276Z","shell.execute_reply":"2023-12-30T22:44:33.160188Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting openai==1.6.1\n  Obtaining dependency information for openai==1.6.1 from https://files.pythonhosted.org/packages/e7/44/5ece9adb8b5943273c845a1e3200168b396f556051b7d2745995abf41584/openai-1.6.1-py3-none-any.whl.metadata\n  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\nCollecting httpcore==1.0.2\n  Obtaining dependency information for httpcore==1.0.2 from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nCollecting httpx==0.26.0\n  Obtaining dependency information for httpx==0.26.0 from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata\n  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\nCollecting typing-extensions==4.9.0\n  Obtaining dependency information for typing-extensions==4.9.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting pydantic==1.9.0\n  Downloading pydantic-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting better-profanity\n  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain\n  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/6a/ca/27a0131c50d4ea8877d6183c7a2aaf16660b17d9b35144bb75a7393639eb/langchain-0.0.353-py3-none-any.whl.metadata\n  Downloading langchain-0.0.353-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.8.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (4.66.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpcore==1.0.2) (2023.11.17)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.0.2) (0.14.0)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx==0.26.0) (3.4)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nCollecting jsonpatch<2.0,>=1.33 (from langchain)\n  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-community<0.1,>=0.0.2 (from langchain)\n  Obtaining dependency information for langchain-community<0.1,>=0.0.2 from https://files.pythonhosted.org/packages/81/ac/4002f920066d13c50d93c3745f8a96c744a9413d2edefbf021dff0e8dcee/langchain_community-0.0.7-py3-none-any.whl.metadata\n  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\nCollecting langchain-core<0.2,>=0.1.4 (from langchain)\n  Obtaining dependency information for langchain-core<0.2,>=0.1.4 from https://files.pythonhosted.org/packages/03/83/b6cb51143388fd12ac9543cde97b61649bf0f2f0cd8ddcaee87750a93781/langchain_core-0.1.4-py3-none-any.whl.metadata\n  Downloading langchain_core-0.1.4-py3-none-any.whl.metadata (4.0 kB)\nCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n  Obtaining dependency information for langsmith<0.1.0,>=0.0.70 from https://files.pythonhosted.org/packages/c0/a2/7814b2341d2919f8305cdaff2e37f76b04c45839f402a38cf13ef7153bea/langsmith-0.0.75-py3-none-any.whl.metadata\n  Downloading langsmith-0.0.75-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.6.1) (1.1.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.4->langchain)\n  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading openai-1.6.1-py3-none-any.whl (225 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nDownloading langchain-0.0.353-py3-none-any.whl (803 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_community-0.0.7-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.4-py3-none-any.whl (205 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: typing-extensions, packaging, jsonpatch, httpcore, better-profanity, pydantic, httpx, openai, langsmith, langchain-core, langchain-community, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed better-profanity-0.7.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 langchain-0.0.353 langchain-community-0.0.7 langchain-core-0.1.4 langsmith-0.0.75 openai-1.6.1 packaging-23.2 pydantic-1.9.0 typing-extensions-4.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"cache_dir = \"./cache\"","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:44:56.228537Z","iopub.execute_input":"2023-12-30T22:44:56.229374Z","iopub.status.idle":"2023-12-30T22:44:56.233864Z","shell.execute_reply.started":"2023-12-30T22:44:56.229336Z","shell.execute_reply":"2023-12-30T22:44:56.232876Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_seq_items', None)\npd.set_option('display.max_colwidth', 500)\npd.set_option('expand_frame_repr', True)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:44:57.536113Z","iopub.execute_input":"2023-12-30T22:44:57.536837Z","iopub.status.idle":"2023-12-30T22:44:57.542267Z","shell.execute_reply.started":"2023-12-30T22:44:57.536797Z","shell.execute_reply":"2023-12-30T22:44:57.541349Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> AI self-commenting-and-moderating</b>\n\n## `CommentatorMoodModeratorAI` - A self moderating system for social media\n\nIn this section we will build an AI system that consists of two LLMs. \n\n-` CommentatorMood` will be an LLM designed to read in a social media post and create a new comment. \n- However, `CommentatorMood` can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. \n- Luckily, that is the role of `Moderator`, the other LLM that will watch what `CommentatorMood` says and flag any negative comments to be removed. \n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:36.998626Z","iopub.execute_input":"2023-12-29T10:22:36.99952Z","iopub.status.idle":"2023-12-29T10:22:37.004082Z","shell.execute_reply.started":"2023-12-29T10:22:36.999482Z","shell.execute_reply":"2023-12-29T10:22:37.002916Z"}}},{"cell_type":"markdown","source":"\n### Step 1 - Building the CommentatorMood Prompt\n\nTo build `CommentatorMood` we will need it to be able to read in the social media post and respond as a commenter. \n\nWe will use engineered prompts to take as an input two things, the first is the social media post and the second is whether or not the comment will have a positive sentiment. \n\nWe'll use a random number generator to create a chance of the flag to be positive or negative in `CommentatorMood` response.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:28:04.874888Z","iopub.execute_input":"2023-12-29T10:28:04.875204Z","iopub.status.idle":"2023-12-29T10:28:05.207142Z","shell.execute_reply.started":"2023-12-29T10:28:04.875174Z","shell.execute_reply":"2023-12-29T10:28:05.205909Z"}}},{"cell_type":"markdown","source":"Prompt template:","metadata":{}},{"cell_type":"code","source":"# Let's start with the prompt template\n\nfrom langchain import PromptTemplate\nimport numpy as np\n\n# Our template for CommentatorMood will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\ncommentator_mood_template = \"\"\"\nYou are a social media post commenter, you will respond to the following post with a {sentiment} response with a limit of 50 words. \nPost:\" {social_post}\"\nComment: \n\"\"\"\n# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\ncommentator_mood_prompt_template = PromptTemplate(\n    input_variables=[\"sentiment\", \"social_post\"],\n    template=commentator_mood_template,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:45:00.231803Z","iopub.execute_input":"2023-12-30T22:45:00.232181Z","iopub.status.idle":"2023-12-30T22:45:00.548653Z","shell.execute_reply.started":"2023-12-30T22:45:00.232151Z","shell.execute_reply":"2023-12-30T22:45:00.547899Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Add parms to promt Template:","metadata":{}},{"cell_type":"code","source":"# Okay now that's ready we need to make the randomized sentiment\nrandom_sentiment = \"mean\"\n# We'll also need our social media post:\nsocial_post = \"I'm learning about LangChain in this Kaggle, I'm having a lot of fun learning! #AI It's super cool !\"\n\n# Let's create the prompt and print it out, this will be given to the LLM.\ncommentator_mood_prompt = commentator_mood_prompt_template.format(\n    sentiment=random_sentiment, \n    social_post=social_post\n)\nprint(f\"Commentator Mood prompt:{commentator_mood_prompt}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:45:01.860496Z","iopub.execute_input":"2023-12-30T22:45:01.860953Z","iopub.status.idle":"2023-12-30T22:45:01.866392Z","shell.execute_reply.started":"2023-12-30T22:45:01.860926Z","shell.execute_reply":"2023-12-30T22:45:01.865460Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Commentator Mood prompt:\nYou are a social media post commenter, you will respond to the following post with a mean response with a limit of 50 words. \nPost:\" I'm learning about LangChain in this Kaggle, I'm having a lot of fun learning! #AI It's super cool !\"\nComment: \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 2 - Building the CommentatorMood LLM\n\nNote: We provide an option for you to use either Hugging Face or OpenAI. If you continue with Hugging Face, the notebook execution will take a long time (up to 10 mins each cell). If you don't mind using OpenAI, following the next markdown cell for API key generation instructions. \n\nFor OpenAI,  we will use their GPT-3 model: `text-babbage-001` as our LLM. \n","metadata":{}},{"cell_type":"markdown","source":"#### OPTIONAL: Use OpenAI's language model\n\nIf you'd rather use OpenAI, you need to generate an OpenAI key. \n\nSteps:\n1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n\nNote: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). \n\n**IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! \n","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"OPENAI_API_KEY\"] = \"<FILL>\"","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:45:06.600162Z","iopub.execute_input":"2023-12-30T22:45:06.600523Z","iopub.status.idle":"2023-12-30T22:45:06.605040Z","shell.execute_reply.started":"2023-12-30T22:45:06.600492Z","shell.execute_reply":"2023-12-30T22:45:06.604036Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # To interact with LLMs in LangChain we need the following modules loaded\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(model=\"text-babbage-001\")\n## We can also use a model from HuggingFaceHub if we wish to go open-source!\n\n#model_id = \"EleutherAI/gpt-neo-2.7B\"\n#tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n#model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir)\n#pipe = pipeline(\n#    \"text-generation\", \n#    model=model, \n#    tokenizer=tokenizer, \n#    max_new_tokens=512, \n#    device_map='auto'\n#)\n#llm = HuggingFacePipeline(pipeline=pipe)\ncommentator_mood_llm = llm","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:45:08.178446Z","iopub.execute_input":"2023-12-30T22:45:08.178900Z","iopub.status.idle":"2023-12-30T22:45:25.558706Z","shell.execute_reply.started":"2023-12-30T22:45:08.178865Z","shell.execute_reply":"2023-12-30T22:45:25.557925Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 3 - Building our Prompt-LLM Chain\n\n\nWe can simplify our input by chaining the prompt template with our LLM so that we can pass the two variables directly to the chain.\n","metadata":{}},{"cell_type":"code","source":"from langchain.chains import LLMChain\nfrom better_profanity import profanity\n\n\ncommentator_mood_chain = LLMChain(\n    llm=commentator_mood_llm,\n    prompt=commentator_mood_prompt_template,\n    output_key=\"commentator_mood_said\",\n    verbose=False,\n)  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n\n# To run our chain we use the .run() command and input our variables as a dict\ncommentator_mood_said = commentator_mood_chain.run(\n    {\n        \"sentiment\": random_sentiment, \n        \"social_post\": social_post\n    }\n)\n\n# Before printing what Commentator Mood said, let's clean it up:\ncleaned_commentator_mood_said = profanity.censor(commentator_mood_said)\nprint(f\"Commentator Mood said:{cleaned_commentator_mood_said}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:46:06.569060Z","iopub.execute_input":"2023-12-30T22:46:06.569423Z","iopub.status.idle":"2023-12-30T22:46:07.264597Z","shell.execute_reply.started":"2023-12-30T22:46:06.569394Z","shell.execute_reply":"2023-12-30T22:46:07.263711Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Commentator Mood said:\nYou are a ****. Learn about LangChain before you start commenting on other people's Kaggles. AI is nothing but a **** gimmick, and you know it.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 4 - Building the second chain for our Moderator","metadata":{}},{"cell_type":"code","source":"#####################################\n# 1 # We will build the prompt template\n# Our template for Moderator will take CommentatorMood's comment and do some sentiment analysis.\nmoderator_template = \"\"\"\nYou are Moderator, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\nOriginal comment: {commentator_mood_said}\nEdited comment:\n\"\"\"\n# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\nmoderator_prompt_template = PromptTemplate(\n    input_variables=[\"commentator_mood_said\"],\n    template=moderator_template,\n)\n\n#####################################\n# 2 # We connect an LLM for Moderator, (we could use a slightly more advanced model 'text-davinci-003 since we have some more logic in this prompt).\n\n#moderator_llm = llm\n# Uncomment the line below if you were to use OpenAI instead\nmoderator_llm = OpenAI(model=\"text-davinci-003\")\n\n#####################################\n# 3 # We build the chain for Moderator\nmoderator_chain = LLMChain(\n    llm=moderator_llm, \n    prompt=moderator_prompt_template, \n    verbose=False\n)  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n\n#####################################\n# 4 # Let's run the chain with what CommentatorMood last said\n# To run our chain we use the .run() command and input our variables as a dict\nmoderator_says = moderator_chain.run({\"commentator_mood_said\": commentator_mood_said})\n# Let's see what moderator said...\nprint(f\"Moderator says: {moderator_says}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:49:07.697894Z","iopub.execute_input":"2023-12-30T22:49:07.698299Z","iopub.status.idle":"2023-12-30T22:49:08.619836Z","shell.execute_reply.started":"2023-12-30T22:49:07.698265Z","shell.execute_reply":"2023-12-30T22:49:08.618807Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Moderator says: ***** ***** ***** ***** ***** ***** Kaggles. AI is nothing but a ***** *****, and you know it.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 5 - Building our first Sequential Chain","metadata":{}},{"cell_type":"code","source":"from langchain.chains import SequentialChain\n\n# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added to the chain. These input variables can be used at any point in the chain, not just the start.\ncommentator_mood_moderator_chain = SequentialChain(\n    chains=[commentator_mood_chain, moderator_chain],\n    input_variables=[\"sentiment\", \"social_post\"],\n    verbose=True,\n)\n\n# We can now run the chain with our randomized sentiment, and the social post!\nchain_commentator_mood_moderator_says = commentator_mood_moderator_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})\nprint(f\"Commentator Mood Moderator said:{chain_commentator_mood_moderator_says}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T22:58:24.319522Z","iopub.execute_input":"2023-12-30T22:58:24.320502Z","iopub.status.idle":"2023-12-30T22:58:25.736065Z","shell.execute_reply.started":"2023-12-30T22:58:24.320464Z","shell.execute_reply":"2023-12-30T22:58:25.735122Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nCommentator Mood Moderator said:***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n","output_type":"stream"}]}]}